title: Bayesian Superiority Estimation with R2D2 Priors: A Practical Guide for Protein Screening
---
author: Eric J. Ma
---
body:

```python
import marimo as mo
```

# Bayesian Superiority Estimation with R2D2 Priors: A Practical Guide for Protein Screening

When screening hundreds of molecules, proteins, or interventions,
two critical questions often arise:

1. Is our experimental setup actually measuring the effect we care about
   (vs. experimental noise)?
2. Which candidates are truly superior to others?

In this tutorial, we'll tackle both challenges using a practical example:
a protein screening experiment with fluorescence readouts.
We'll show how **R2D2 priors** help interpret variance decomposition,
and how **Bayesian superiority calculation**
enables robust ranking of candidates.
Both techniques generalize to drug discovery, material science,
or any domain requiring rigorous comparison of multiple alternatives.

## The Protein Screening Example

Let's consider a dataset with fluorescence measurements for over 100 proteins
across multiple experiments and replicates.

Our experimental design includes:

- A control protein present in all experiments and replicates
- "Crossover" proteins measured across all experiments
- Unique test proteins in each experiment

This design is common in high-throughput screening scenarios
where measuring all proteins in all conditions is impractical.
For simplicity, I am leaving out factors such as plate and well position,
but know that in a real life situation,
these factors would be considered as part of the experimental design.

## Importing PyMC for Bayesian Modeling

We'll use PyMC to implement our Bayesian hierarchical model with R2D2 priors.
PyMC is a powerful probabilistic programming framework
that allows us to define and sample from complex statistical models.

```python
import pymc as pm
```

## Protein Fluorescence Dataset Description

This dataset contains fluorescence measurements
from a series of protein experiments.
The experimental design is as follows:

### Structure

- Total proteins measured: >100 unique proteins
- Number of experiments: 3
- Replicates per experiment: 2
- Total measurements: ~300-400

### Key Components

1. Control protein: Present in all experiments and replicates
2. Crossover proteins: 3-5 proteins measured across all experiments
3. Test proteins: Unique to each experiment

### Experimental Variation

- Inter-experiment variation: Higher systematic shifts
- Intra-experiment variation (between replicates): Lower systematic shifts

### Measurement Details

- Readout: Fluorescent units
- Each protein appears in either:
  * Multiple measurements (control and crossover proteins)
  * Single measurement (test proteins)

### Purpose

This design allows for:

- Quality control through control protein measurements
- Cross-experiment normalization using crossover proteins
- High-throughput screening of many unique proteins

## Generating Synthetic Protein Fluorescence Data

To demonstrate our approach,
we'll generate synthetic data
that mimics a realistic protein screening experiment.
Accordingly, we will:

1. Define the experimental structure (experiments, replicates, proteins)
2. Create protein identifiers for controls, crossovers, and test proteins
3. Simulate "true" underlying protein activities
4. Add systematic experiment and replicate effects
5. Incorporate measurement noise

The code below creates a dataset
that captures key features of real protein screening experiments,
including batch effects between experiments and replicates.

```python
import numpy as np
import pandas as pd

# Set random seed for reproducibility
np.random.seed(42)

# Define parameters
n_experiments = 3
n_replicates = 2
n_proteins_per_exp = 40
n_crossover = 4

# Create protein names
control = ["Control"]
crossover_proteins = [f"Crossover_{i}" for i in range(n_crossover)]
other_proteins = [f"Protein_{i}" for i in range(100)]

# Base fluorescence values
base_values = {}
base_values["Control"] = 1000
for p in crossover_proteins:
    base_values[p] = np.random.normal(1000, 200)
for p in other_proteins:
    base_values[p] = np.random.normal(1000, 200)

# Create experiment effects
exp_effects = np.random.normal(1, 0.3, n_experiments)
rep_effects = np.random.normal(1, 0.1, (n_experiments, n_replicates))

# Generate data
data = []
for exp in range(n_experiments):
    # Select proteins for this experiment
    exp_proteins = (
        control + crossover_proteins + other_proteins[exp * 30 : (exp + 1) * 30]
    )

    for rep in range(n_replicates):
        for protein in exp_proteins:
            # Add noise and effects
            value = (
                base_values[protein]
                * exp_effects[exp]
                * rep_effects[exp, rep]
                * np.random.normal(1, 0.05)
            )

            data.append(
                {
                    "Experiment": f"Exp_{exp+1}",
                    "Replicate": f"Rep_{rep+1}",
                    "Protein": protein,
                    "Fluorescence": value,
                }
            )

# Convert to DataFrame
df = pd.DataFrame(data)
df
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Experiment</th>
      <th>Replicate</th>
      <th>Protein</th>
      <th>Fluorescence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Control</td>
      <td>1087.476281</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Crossover_0</td>
      <td>1054.176224</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Crossover_1</td>
      <td>955.647739</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Crossover_2</td>
      <td>1091.751188</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Crossover_3</td>
      <td>1189.344109</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>205</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_85</td>
      <td>1765.149428</td>
    </tr>
    <tr>
      <th>206</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_86</td>
      <td>1595.422298</td>
    </tr>
    <tr>
      <th>207</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_87</td>
      <td>1889.585595</td>
    </tr>
    <tr>
      <th>208</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_88</td>
      <td>1394.395041</td>
    </tr>
    <tr>
      <th>209</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_89</td>
      <td>1411.831297</td>
    </tr>
  </tbody>
</table>
<p>210 rows Ã— 4 columns</p>
</div>

In the synthetic dataset we've created, we simulated:

- 3 experiments with 2 replicates each
- A control protein and 4 crossover proteins present in all experiments
- 100 other proteins distributed across experiments
- Multiplicative experiment effects (mean=1, sd=0.3)
- Multiplicative replicate effects (mean=1, sd=0.1)
- Multiplicative measurement noise (mean=1, sd=0.05)

This structure simulates a typical screening setup
where batch effects between experiments are stronger than replicate variation,
and both contribute significantly to the observed fluorescence values.
The setting mirrors real experimental challenges
where we need to separate biological signal from technical noise.

## Examining the Raw Data

Before modeling,
let's visualize the raw data for control and crossover proteins
to understand the experimental variation:

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Filter for Control and Crossover samples
mask = df["Protein"].str.contains("Control|Crossover")
filtered_df = df[mask]

# Create the swarm plot
sns.swarmplot(data=filtered_df, x="Experiment", y="Fluorescence", hue="Protein", size=8)

plt.title("Activity for Controls and Crossover Samples")
plt.ylabel("Fluorescence")
plt.gca()
```

![png](protein_estimation_10_1.webp)

Notice the dramatic shift in fluorescence values across experiments.
Experiment 3 shows substantially higher fluorescence readings
(around 1500-2000 units)
compared to Experiments 1 and 2 (mostly below 1300 units).
This systematic shift affects all proteins,
including our control and crossover samples.

This pattern illustrates a common challenge in high-throughput screening:
significant batch effects between experiments
that can mask the true biological signal we're interested in.
Without accounting for these experimental factors,
we might incorrectly attribute higher activity to proteins
simply because they were measured in Experiment 3.

Additionally, there's variation between replicates within each experiment,
though this effect is smaller than the between-experiment variation.
Our modeling approach needs to account for
both sources of experimental noise
to accurately compare proteins across the entire dataset.

## The R2D2 Prior: Interpretable Variance Decomposition

To address these experimental batch effects
and properly separate biological signal from noise,
we need a modeling approach
that explicitly accounts for different sources of variation.
This is where the R2D2 prior becomes valuable.

The R2D2 prior (R-squared Dirichlet decomposition)
provides an interpretable framework for variance decomposition
by placing a prior on the Bayesian coefficient of determination ($R^2$),
which then induces priors on individual parameters.
Introduced by [Yanchenko, Bondell, and Reich (2021)](https://arxiv.org/abs/2111.10718),
the R2D2 prior is especially valuable
for generalized linear mixed models
where we want to understand how much of the total variance
is explained by different model components.

The core idea of R2D2 is to:

1. Place a $\text{Beta}(a,b)$ prior on the coefficient of determination ($R^2$). An uninformative prior, such as a $\text{Beta}(1,1)$ is appropriate.
2. This induces a prior on the global variance parameter (`global_var` in our model, which represents the total variance of the linear predictor)
3. The global variance is then decomposed into component-specific variances via a Dirichlet-distributed vector, each mapping to a particular variance component.

This approach offers a significant advantage over traditional hierarchical models where we would specify separate, unrelated priors for each variance component (e.g., $\sigma^2_{experiment}$, $\sigma^2_{replicate}$, $\sigma^2_{protein}$). Those traditional approaches often rely on arbitrary "magic numbers" for each variance parameter, making it difficult to interpret how much each component contributes to the overall model fit. With R2D2, we instead control the total explained variance through a single interpretable parameter ($R^2$) and then partition that variance meaningfully through the Dirichlet distribution, creating a coherent framework for understanding variance decomposition. And to top it off, the hierarchical nature of variance specification here ensures regularization of variance estimates away from unreasonable values.

Here's how we implement the model:

```python
def _():
    # Create categorical indices
    exp_idx = pd.Categorical(df["Experiment"]).codes
    rep_idx = pd.Categorical(df["Replicate"]).codes
    prot_idx = pd.Categorical(df["Protein"]).codes

    # Define coordinates for dimensions
    coords = {
        "experiment": df["Experiment"].unique(),
        "replicate": df["Replicate"].unique(),
        "protein": df["Protein"].unique(),
    }

    with pm.Model(coords=coords) as model:
        # Explicitly define RÂ² prior (core of R2D2)
        r_squared = pm.Beta("r_squared", alpha=1, beta=1)

        # Global parameters
        global_mean = pm.Normal(
            "global_mean", mu=7, sigma=1
        )  # log scale for fluorescence

        # Residual variance (unexplained)
        sigma_squared = pm.HalfNormal("sigma_squared", sigma=1)

        # Global variance derived from RÂ² and residual variance
        # For normal models: W = sigmaÂ² * rÂ²/(1-rÂ²)
        global_var = pm.Deterministic(
            "global_var", sigma_squared * r_squared / (1 - r_squared)
        )
        global_sd = pm.Deterministic("global_sd", pm.math.sqrt(global_var))

        # R2D2 decomposition parameters
        # 4 components: experiment, replicate (nested in experiment), protein, and unexplained
        props = pm.Dirichlet("props", a=np.ones(4))

        # Component variances (for interpretability)
        exp_var = pm.Deterministic("exp_var", props[0] * global_var)
        rep_var = pm.Deterministic("rep_var", props[1] * global_var)
        prot_var = pm.Deterministic("prot_var", props[2] * global_var)
        unexplained_var = pm.Deterministic("unexplained_var", props[3] * global_var)

        # Component standard deviations
        exp_sd = pm.Deterministic("exp_sd", pm.math.sqrt(exp_var))
        rep_sd = pm.Deterministic("rep_sd", pm.math.sqrt(rep_var))
        prot_sd = pm.Deterministic("prot_sd", pm.math.sqrt(prot_var))
        unexplained_sd = pm.Deterministic(
            "unexplained_sd", pm.math.sqrt(unexplained_var)
        )

        # Component effects
        exp_effect = pm.Normal("exp_effect", mu=0, sigma=exp_sd, dims="experiment")
        rep_effect = pm.Normal(
            "rep_effect", mu=0, sigma=rep_sd, dims=("experiment", "replicate")
        )
        prot_effect = pm.Normal("prot_effect", mu=0, sigma=prot_sd, dims="protein")

        # Protein activity (what we're ultimately interested in)
        prot_activity = pm.Deterministic(
            "prot_activity", global_mean + prot_effect, dims="protein"
        )

        # Expected value
        y_hat = (
            global_mean
            + exp_effect[exp_idx]
            + rep_effect[exp_idx, rep_idx]
            + prot_effect[prot_idx]
        )

        # Calculate model RÂ² directly (for verification)
        model_r2 = pm.Deterministic(
            "model_r2",
            (exp_var + rep_var + prot_var)
            / (exp_var + rep_var + prot_var + unexplained_var),
        )

        # Likelihood
        y = pm.Normal(
            "y", mu=y_hat, sigma=unexplained_sd, observed=np.log(df["Fluorescence"])
        )

        # Sample
        trace = pm.sample(
            2000, tune=1000, return_inferencedata=True, nuts_sampler="nutpie"
        )
    return model, trace


model, trace = _()
```

## Model Convergence and Posterior Analysis

Now that we've fitted our Bayesian model with R2D2 priors, we need to assess:

1. Whether the model converged properly
2. How the variance is partitioned across different components
3. What protein activities look like after accounting for experimental effects

First, let's check for divergent transitions, which can indicate problems with the model fitting process.

```python
trace.sample_stats.diverging.sum()
```

<div><svg style="position: absolute; width: 0; height: 0; overflow: hidden">
<defs>
<symbol id="icon-database" viewBox="0 0 32 32">
<path d="M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z"></path>
<path d="M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
<path d="M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
</symbol>
<symbol id="icon-file-text2" viewBox="0 0 32 32">
<path d="M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z"></path>
<path d="M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
<path d="M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
<path d="M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
</symbol>
</defs>
</svg>
<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.
 *
 */

:root {
  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));
  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));
  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));
  --xr-border-color: var(--jp-border-color2, #e0e0e0);
  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);
  --xr-background-color: var(--jp-layout-color0, white);
  --xr-background-color-row-even: var(--jp-layout-color1, white);
  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);
}

html[theme="dark"],
html[data-theme="dark"],
body[data-theme="dark"],
body.vscode-dark {
  --xr-font-color0: rgba(255, 255, 255, 1);
  --xr-font-color2: rgba(255, 255, 255, 0.54);
  --xr-font-color3: rgba(255, 255, 255, 0.38);
  --xr-border-color: #1f1f1f;
  --xr-disabled-color: #515151;
  --xr-background-color: #111111;
  --xr-background-color-row-even: #111111;
  --xr-background-color-row-odd: #313131;
}

.xr-wrap {
  display: block !important;
  min-width: 300px;
  max-width: 700px;
}

.xr-text-repr-fallback {
  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */
  display: none;
}

.xr-header {
  padding-top: 6px;
  padding-bottom: 6px;
  margin-bottom: 4px;
  border-bottom: solid 1px var(--xr-border-color);
}

.xr-header > div,
.xr-header > ul {
  display: inline;
  margin-top: 0;
  margin-bottom: 0;
}

.xr-obj-type,
.xr-array-name {
  margin-left: 2px;
  margin-right: 10px;
}

.xr-obj-type {
  color: var(--xr-font-color2);
}

.xr-sections {
  padding-left: 0 !important;
  display: grid;
  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;
}

.xr-section-item {
  display: contents;
}

.xr-section-item input {
  display: inline-block;
  opacity: 0;
  height: 0;
}

.xr-section-item input + label {
  color: var(--xr-disabled-color);
}

.xr-section-item input:enabled + label {
  cursor: pointer;
  color: var(--xr-font-color2);
}

.xr-section-item input:focus + label {
  border: 2px solid var(--xr-font-color0);
}

.xr-section-item input:enabled + label:hover {
  color: var(--xr-font-color0);
}

.xr-section-summary {
  grid-column: 1;
  color: var(--xr-font-color2);
  font-weight: 500;
}

.xr-section-summary > span {
  display: inline-block;
  padding-left: 0.5em;
}

.xr-section-summary-in:disabled + label {
  color: var(--xr-font-color2);
}

.xr-section-summary-in + label:before {
  display: inline-block;
  content: "â–º";
  font-size: 11px;
  width: 15px;
  text-align: center;
}

.xr-section-summary-in:disabled + label:before {
  color: var(--xr-disabled-color);
}

.xr-section-summary-in:checked + label:before {
  content: "â–¼";
}

.xr-section-summary-in:checked + label > span {
  display: none;
}

.xr-section-summary,
.xr-section-inline-details {
  padding-top: 4px;
  padding-bottom: 4px;
}

.xr-section-inline-details {
  grid-column: 2 / -1;
}

.xr-section-details {
  display: none;
  grid-column: 1 / -1;
  margin-bottom: 5px;
}

.xr-section-summary-in:checked ~ .xr-section-details {
  display: contents;
}

.xr-array-wrap {
  grid-column: 1 / -1;
  display: grid;
  grid-template-columns: 20px auto;
}

.xr-array-wrap > label {
  grid-column: 1;
  vertical-align: top;
}

.xr-preview {
  color: var(--xr-font-color3);
}

.xr-array-preview,
.xr-array-data {
  padding: 0 5px !important;
  grid-column: 2;
}

.xr-array-data,
.xr-array-in:checked ~ .xr-array-preview {
  display: none;
}

.xr-array-in:checked ~ .xr-array-data,
.xr-array-preview {
  display: inline-block;
}

.xr-dim-list {
  display: inline-block !important;
  list-style: none;
  padding: 0 !important;
  margin: 0;
}

.xr-dim-list li {
  display: inline-block;
  padding: 0;
  margin: 0;
}

.xr-dim-list:before {
  content: "(";
}

.xr-dim-list:after {
  content: ")";
}

.xr-dim-list li:not(:last-child):after {
  content: ",";
  padding-right: 5px;
}

.xr-has-index {
  font-weight: bold;
}

.xr-var-list,
.xr-var-item {
  display: contents;
}

.xr-var-item > div,
.xr-var-item label,
.xr-var-item > .xr-var-name span {
  background-color: var(--xr-background-color-row-even);
  margin-bottom: 0;
}

.xr-var-item > .xr-var-name:hover span {
  padding-right: 5px;
}

.xr-var-list > li:nth-child(odd) > div,
.xr-var-list > li:nth-child(odd) > label,
.xr-var-list > li:nth-child(odd) > .xr-var-name span {
  background-color: var(--xr-background-color-row-odd);
}

.xr-var-name {
  grid-column: 1;
}

.xr-var-dims {
  grid-column: 2;
}

.xr-var-dtype {
  grid-column: 3;
  text-align: right;
  color: var(--xr-font-color2);
}

.xr-var-preview {
  grid-column: 4;
}

.xr-index-preview {
  grid-column: 2 / 5;
  color: var(--xr-font-color2);
}

.xr-var-name,
.xr-var-dims,
.xr-var-dtype,
.xr-preview,
.xr-attrs dt {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  padding-right: 10px;
}

.xr-var-name:hover,
.xr-var-dims:hover,
.xr-var-dtype:hover,
.xr-attrs dt:hover {
  overflow: visible;
  width: auto;
  z-index: 1;
}

.xr-var-attrs,
.xr-var-data,
.xr-index-data {
  display: none;
  background-color: var(--xr-background-color) !important;
  padding-bottom: 5px !important;
}

.xr-var-attrs-in:checked ~ .xr-var-attrs,
.xr-var-data-in:checked ~ .xr-var-data,
.xr-index-data-in:checked ~ .xr-index-data {
  display: block;
}

.xr-var-data > table {
  float: right;
}

.xr-var-name span,
.xr-var-data,
.xr-index-name div,
.xr-index-data,
.xr-attrs {
  padding-left: 25px !important;
}

.xr-attrs,
.xr-var-attrs,
.xr-var-data,
.xr-index-data {
  grid-column: 1 / -1;
}

dl.xr-attrs {
  padding: 0;
  margin: 0;
  display: grid;
  grid-template-columns: 125px auto;
}

.xr-attrs dt,
.xr-attrs dd {
  padding: 0;
  margin: 0;
  float: left;
  padding-right: 10px;
  width: auto;
}

.xr-attrs dt {
  font-weight: normal;
  grid-column: 1;
}

.xr-attrs dt:hover span {
  display: inline-block;
  background: var(--xr-background-color);
  padding-right: 10px;
}

.xr-attrs dd {
  grid-column: 2;
  white-space: pre-wrap;
  word-break: break-all;
}

.xr-icon-database,
.xr-icon-file-text2,
.xr-no-icon {
  display: inline-block;
  vertical-align: middle;
  width: 1em;
  height: 1.5em !important;
  stroke-width: 0;
  stroke: currentColor;
  fill: currentColor;
}
</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;diverging&#x27; ()&gt; Size: 8B
array(0)</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'diverging'</div></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-bc9876eb-e0ee-4bbc-9bea-be17a2b60bab' class='xr-array-in' type='checkbox' checked><label for='section-bc9876eb-e0ee-4bbc-9bea-be17a2b60bab' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0</span></div><div class='xr-array-data'><pre>array(0)</pre></div></div></li><li class='xr-section-item'><input id='section-b28d61e9-7b66-4764-a000-a9a5eef2d870' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-b28d61e9-7b66-4764-a000-a9a5eef2d870' class='xr-section-summary'  title='Expand/collapse section'>Coordinates: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-2ca778a5-1cfd-41f5-9e28-66508680bb98' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-2ca778a5-1cfd-41f5-9e28-66508680bb98' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-9728e755-fea4-4713-b1d7-ae704531c897' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-9728e755-fea4-4713-b1d7-ae704531c897' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>

Zero divergent transitions indicates good model convergence. This means our posterior samples should provide reliable estimates of all model parameters.

Next, let's examine the posterior distributions of the variance components. The `props` parameter from our model represents how the total variance is partitioned across experiment, replicate, protein, and unexplained components.


```python
import arviz as az

axes_posterior_props = az.plot_posterior(trace, var_names=["props"])
axes_posterior_props[0].set_title("experiment")
axes_posterior_props[1].set_title("replicate")
axes_posterior_props[2].set_title("protein")
axes_posterior_props[3].set_title("unexplained")
```

![png](protein_estimation_17_1.webp)

These posterior plots show the distributions of the variance components. Each represents the proportion of total variance attributed to that component. We can see clear differences in the contributions from each source.

We can also look at the total $R^2$ value, which represents the proportion of variance explained by the model:


```python
az.plot_posterior(trace, var_names=["model_r2"])
```

![png](protein_estimation_19_1.webp)

Let's examine the posterior distributions of each variance component. In the ridgeline plot below, we see that unexplained variance currently constitutes a tiny fraction of total variation, while experiment and replicate -- two components that really should not contribute much to the output variation, are actually significant contributors to the readout variation. This can serve as a metric on laboratory consistency. Ideally, the protein that we're engineering should contribute the most variation to the output that we see.

The R2D2 prior answers our first critical question in a way that guides practical action: we should focus on improving experimental execution to make our experiments more consistent. Ideally, the protein effect should be the majority contributor to the readout variation. This analysis suggests that refining our experimental protocol to reduce batch effects between experiments and variability between replicates would substantially improve the signal-to-noise ratio of our screening platform.

```python
import joypy

# Extract the posterior samples for props
props_samples = trace.posterior["props"].values.reshape(-1, 4)
props_df = pd.DataFrame(
    props_samples, columns=["Experiment", "Replicate", "Protein", "Unexplained"]
)

# Create the ridgeline plot
fig, axes = joypy.joyplot(
    props_df,
    figsize=(10, 4),
    colormap=plt.cm.tab10,
    alpha=0.7,
    title="Posterior Distributions of Variance Components",
)

# Add vertical lines for means
for i, col in enumerate(props_df.columns):
    axes[i].axvline(props_df[col].mean(), color="black", linestyle="--", alpha=0.8)
    axes[i].text(
        props_df[col].mean() + 0.02,
        0.5,
        f"Mean: {props_df[col].mean():.2f}",
        transform=axes[i].get_yaxis_transform(),
    )

plt.tight_layout()
plt.gca()
```

![png](protein_estimation_21_1.webp)

## Examining Protein Activity Estimates

Now that we've decomposed the variance and accounted for experimental effects, we can examine the actual protein activity estimates. These represent the "true" biological signal after removing the experimental noise:


```python
ax = az.plot_forest(trace.posterior["prot_activity"])[0]
ax.set_xlabel("log(protein activity)")
```

![png](protein_estimation_23_1.webp)

This forest plot displays posterior distributions of protein activity on the log scale, with horizontal lines representing 94% credible intervals. We can identify several proteins (such as Protein_12, Protein_38, and Protein_66) that appear to have higher activity than others.

However, a critical challenge emerges: despite most proteins having similar uncertainty in their estimates (shown by consistent credible interval widths), this uncertainty creates significant ambiguity when comparing proteins with similar point estimates. Simply ranking proteins by their posterior mean activity could lead us to prioritize proteins with slightly higher point estimates when overlapping uncertainty makes it difficult to confidently determine their true superiority.

This illustrates a fundamental limitation of ranking by point estimates alone: it fails to properly account for uncertainty across different proteins. A protein with a slightly lower mean but narrower credible intervals might actually be a better candidate than one with a higher mean but wider uncertainty bounds.

While the forest plot offers a comprehensive overview of all protein activities, making definitive comparisons remains difficult due to overlapping credible intervals. This is why we need the Bayesian superiority calculation in the next section - to properly quantify the probability that one protein truly outperforms another while fully accounting for uncertainty in our estimates.

## Why not just use effect sizes?

A natural question arises: if we want to compare proteins, why not simply calculate effect sizes based on posterior activity estimates?

Effect sizes (like Cohen's d or standardized mean differences) are often used in traditional statistical analyses to quantify the magnitude of differences between groups. In our Bayesian context, we could calculate effect sizes between each pair of proteins using our posterior activity estimates.

Let's illustrate this approach by calculating standardized effect sizes between a reference protein (the control) and all other proteins:

```python
def plot_effect_sizes(proteins_to_plot: list[str] = None, num_to_plot: int = None):
    # Get posterior samples of protein activities
    prot_activity = trace.posterior["prot_activity"].values

    # Reshape to (samples, proteins)
    n_samples = prot_activity.shape[0] * prot_activity.shape[1]
    n_proteins = prot_activity.shape[2]
    prot_activity_flat = prot_activity.reshape(-1, n_proteins)

    # Get protein names
    protein_names = trace.posterior["protein"].values

    # Find index of control protein
    control_idx = np.where(protein_names == "Control")[0][0]

    # Calculate effect sizes for each posterior sample
    # Effect size = (protein_activity - control_activity) / pooled_std
    effect_sizes = np.zeros((n_samples, n_proteins))

    for i in range(n_samples):
        control_value = prot_activity_flat[i, control_idx]
        for j in range(n_proteins):
            if j != control_idx:
                # Calculate effect size for this sample
                effect = prot_activity_flat[i, j] - control_value
                # Using a simplified effect size calculation (mean difference)
                # In practice, you might use a pooled standard deviation
                effect_sizes[i, j] = effect

    # Create a DataFrame with effect size statistics
    effect_size_stats = pd.DataFrame(
        {
            "Protein": protein_names,
            "Mean_Effect": np.mean(effect_sizes, axis=0),
            "Lower_CI": np.percentile(effect_sizes, 2.5, axis=0),
            "Upper_CI": np.percentile(effect_sizes, 97.5, axis=0),
        }
    )

    # Filter proteins if specified
    if proteins_to_plot is not None:
        effect_size_stats = effect_size_stats[
            effect_size_stats["Protein"].isin(proteins_to_plot)
        ]

    # Sort by mean effect
    top_proteins = effect_size_stats.sort_values("Mean_Effect", ascending=False)

    if num_to_plot:
        top_proteins = top_proteins.head(num_to_plot)

    plt.figure(figsize=(10, 0.5 * len(top_proteins)))

    # Create scatter plot with error bars
    plt.errorbar(
        x=top_proteins["Mean_Effect"],
        y=range(len(top_proteins)),
        xerr=np.vstack(
            [
                top_proteins["Mean_Effect"] - top_proteins["Lower_CI"],
                top_proteins["Upper_CI"] - top_proteins["Mean_Effect"],
            ]
        ),
        fmt="o",
        capsize=5,
    )

    plt.yticks(range(len(top_proteins)), top_proteins["Protein"])
    plt.axvline(x=0, color="gray", linestyle="--")
    plt.xlabel("Effect Size (difference from Control)")
    plt.title("Posterior Effect Sizes of Top Proteins vs Control")
    sns.despine()
    return plt.gca(), effect_sizes


axes_effect_sizes, effect_sizes = plot_effect_sizes(num_to_plot=20)
axes_effect_sizes
```

![png](protein_estimation_26_1.webp)

This plot shows the posterior distribution of effect sizes for the top proteins compared to the control. The horizontal lines represent 95% credible intervals.

What's particularly interesting is Crossover 3, Protein 51, and Protein 66. Let's focus on them.

- Between Crossover 3 and Protein 51, is Protein 51 genuinely worse than Crossover 3? It's hard to tell.
- Between Crossover 3 and Protein 66, is Protein 66 genuinely better than Crossover 3? It's also hard to tell.


```python
axes_effect_sizes_filtered, _ = plot_effect_sizes(
    proteins_to_plot=["Crossover_3", "Protein_51", "Protein_66"]
)
axes_effect_sizes_filtered
```

![png](protein_estimation_28_1.webp)

While effect sizes are useful for quantifying the magnitude of differences, there are several important limitations to this approach:

1. **Still a posterior distribution**: Notice that effect sizes themselves have a posterior distribution, not just a single value. The effect size calculation doesn't resolve our uncertainty - it merely transforms it.
2. **Arbitrary reference**: Any effect size calculation requires choosing a reference protein (here, we used the control). The results would differ if we chose a different reference.
3. **Scale dependence**: The interpretation of what constitutes a "large" effect size varies by context and can be subjective.
4. **Doesn't directly answer "which is better"**: Effect sizes tell us about the magnitude of differences, but not directly about the probability that one protein is superior to another.

## Probability of superiority

This is where the probability of superiority calculation shines: it integrates over the entire posterior distribution to produce a single, interpretable number that directly answers our key question: "What is the probability that protein A is better than protein B?"

The probability of superiority is calculated by:

1. For each posterior sample, compare protein A's activity to protein B's
2. Count the proportion of samples where A > B
3. This gives us P(A > B) - a single number from 0 to 1

This approach:

1. **Integrates over uncertainty**: Uses the full posterior distribution, not just point estimates
2. **Directly answers our question**: Tells us the probability of superiority, not just a difference magnitude
3. **Avoids arbitrary references**: Can compare any two proteins directly
4. **Produces an interpretable metric**: A probability from 0 to 1 is intuitive for decision-making

Let's illustrate how the superiority probability integrates over the posterior by looking at the comparison between two specific proteins:

```python
def _():
    # Get posterior samples
    prot_activity = trace.posterior["prot_activity"].values
    prot_activity_flat = prot_activity.reshape(-1, prot_activity.shape[2])

    # Get protein names
    protein_names = trace.posterior["protein"].values

    # Choose two proteins to compare
    protein1 = "Protein_12"  # A high performer
    protein2 = "Protein_66"  # Another high performer

    idx1 = np.where(protein_names == protein1)[0][0]
    idx2 = np.where(protein_names == protein2)[0][0]

    # Extract their posterior samples
    samples1 = prot_activity_flat[:, idx1]
    samples2 = prot_activity_flat[:, idx2]

    # Calculate differences for each posterior sample
    differences = samples1 - samples2

    # Calculate superiority probability
    prob_superiority = np.mean(differences > 0)

    # Plot the posterior of differences
    plt.figure(figsize=(10, 6))

    # Histogram of differences
    sns.histplot(differences, bins=30, alpha=0.6)

    # Add vertical line at zero
    plt.axvline(x=0, color="r", linestyle="--")

    # Shade the area where protein1 > protein2
    positive_mask = differences > 0
    plt.fill_between(
        np.sort(differences[positive_mask]),
        0,
        plt.gca().get_ylim()[1] / 2,  # Half height for visibility
        alpha=0.3,
        color="green",
        label=f"P({protein1} > {protein2}) = {prob_superiority:.3f}",
    )

    plt.xlabel(f"Activity Difference ({protein1} - {protein2})")
    plt.ylabel("Frequency")
    plt.title(
        f"Posterior Distribution of Activity Difference Between {protein1} and {protein2}"
    )
    return plt.legend()


_()
```

![png](protein_estimation_31_1.webp)

This visualization demonstrates the core concept behind the probability of superiority calculation. The green shaded area represents the proportion of posterior samples where the first protein outperforms the second. This proportion (in this case, approximately 0.6) is the probability of superiority.

Rather than reducing our rich posterior distributions to point estimates or even to effect size distributions that still require interpretation, the superiority probability directly integrates over all our uncertainty to answer the precise question we care about: "How likely is it that this protein is better than that one?"

This approach is particularly valuable when:

- Uncertainty differs between proteins
- Point estimates are similar but distributions differ in shape
- You need a clear decision metric for ranking or selection

Next, we'll see how to calculate this for all pairs of proteins to create a comprehensive superiority matrix.

## Examining Replicate Effects

Before moving to protein superiority, let's quickly look at the replicate effects across experiments. This helps us understand the magnitude of technical variation within each experiment.

```python
az.plot_forest(trace.posterior["rep_effect"], rope=[-0.05, 0.05])
```

![png](protein_estimation_34_1.webp)

The forest plot shows the replicate effects across different experiments. The "ROPE" (Region of Practical Equivalence) from -0.05 to 0.05 helps identify which effects are practically significant. Replicate effects that include the ROPE in their credible intervals might be considered negligible for practical purposes.

Now that we've examined both replicate effects and explored why effect sizes are insufficient for robust protein comparison, we're ready to implement the Bayesian superiority calculation we just described - a method that integrates over the full posterior distribution to determine which proteins truly outperform others.

## Bayesian Superiority Calculation

Now that we've properly modeled the sources of variation and examined the protein activities, we can tackle our second question: which proteins are truly superior?

The traditional approach would be to compare point estimates (means) and judge significance using p-values. But the Bayesian approach offers something more powerful: the probability that one protein is superior to another, calculated directly from posterior samples.

Here's how we calculate a "superiority matrix":

```python
from tqdm.auto import tqdm


def _():
    n_proteins = trace.posterior["prot_activity"].shape[-1]
    prot_activity = trace.posterior["prot_activity"].values.reshape(-1, n_proteins)

    superiority_matrix = np.zeros((n_proteins, n_proteins))

    for i in tqdm(range(n_proteins)):
        for j in range(n_proteins):
            if i != j:
                superiority_matrix[i, j] = np.mean(
                    prot_activity[:, i] > prot_activity[:, j]
                )
    return superiority_matrix


superiority_matrix = _()
```

This superiority matrix gives us, for each pair of proteins (i, j), the probability that protein i has higher activity than protein j, estimated from our posterior samples. This calculation incorporates all the uncertainty in our model.

The calculation is straightforward but powerful:

1. For each pair of proteins (i,j)
2. For each posterior sample, check if protein i's activity exceeds protein j's
3. Calculate the proportion of samples where i > j

This gives us a probability interpretation: "There's an 85% chance that protein A is superior to protein B" rather than the frequentist approach of "protein A is significantly better than protein B with p<0.05."

Let's visualize this matrix:

```python
# Create heatmap
sns.heatmap(superiority_matrix, annot=True, cmap="YlOrRd", fmt=".2f")
plt.title("Superiority Matrix")
plt.gca()
```

![png](protein_estimation_39_1.webp)

The heatmap shows the pairwise superiority probabilities between all proteins. Brighter colors indicate higher probabilities that the row protein is superior to the column protein.

We can also look at the column-wise mean of the superiority matrix, which gives us a different perspective on protein performance:

```python
sns.heatmap(superiority_matrix.mean(axis=0).reshape(-1, 1))
```

![png](protein_estimation_41_1.webp)

We can rank proteins by their average probability of superiority. While this may seem disingenuous, given that I just ranted against point estimates, it gives us at least one meaningful basis of comparison. Let's plot both the average probability of superiority and the full underlying pairwise comparisons.

```python
def _():
    # Calculate average probability of superiority and sort proteins
    avg_superiority = superiority_matrix.mean(axis=1)
    protein_names = df["Protein"].unique()
    superiority_df = pd.DataFrame(
        {"Protein": protein_names, "Avg_Superiority": avg_superiority}
    )
    sorted_superiority = superiority_df.sort_values(
        "Avg_Superiority", ascending=False
    ).head(20)

    # Create plot
    plt.figure(figsize=(12, 6))

    # For each protein, plot individual points and mean line
    for i, protein in enumerate(sorted_superiority["Protein"]):
        protein_idx = np.where(protein_names == protein)[0][0]
        protein_probs = superiority_matrix[protein_idx]
        plt.scatter([i] * len(protein_probs), protein_probs, alpha=0.5, color="blue")
        plt.hlines(avg_superiority[protein_idx], i - 0.25, i + 0.25, color="red")

    plt.xticks(
        range(len(sorted_superiority)), sorted_superiority["Protein"], rotation=90
    )
    plt.ylabel("Probability of Superiority")
    plt.title("Distribution of Superiority Probabilities by Protein")
    plt.tight_layout()
    sns.despine()
    return plt.gca()


_()
```

![png](protein_estimation_43_1.webp)

Looking at our results, some proteins emerge as the clear top performers, followed by others with moderate superiority. This ranking is particularly valuable because it differs from what we might conclude by simply examining forest plots of the posterior distributions. In forest plots, we would only see the estimated activity levels and their uncertainty intervals, which might lead us to favor proteins with high mean activity but also high uncertainty. The superiority metric, in contrast, directly quantifies the probability that one protein outperforms others, properly accounting for the full posterior distribution and the uncertainty in each comparison.

## Relationship Between Protein Activity and Superiority

To better understand how protein activity relates to superiority probability, let's create a scatter plot that compares:

1. The posterior mean protein activity (x-axis)
2. Two measures of superiority probability (y-axis):
   - Mean probability of superiority
   - Minimum probability of superiority (a conservative estimate)

This will help identify proteins that are consistently superior versus those that have high mean activity but with high uncertainty.

```python
def _():
    # Calculate probability of superiority distribution for each protein
    superiority_dist = [
        np.concatenate([superiority_matrix[i, :j], superiority_matrix[i, j + 1 :]])
        for i, j in enumerate(range(len(superiority_matrix)))
    ]

    # Get protein activity statistics from trace
    protein_activity_mean = (
        trace.posterior["prot_activity"].mean(dim=["chain", "draw"]).values
    )

    # Create scatter plot
    plt.figure(figsize=(10, 6))

    # Plot points
    plt.scatter(
        protein_activity_mean,
        [np.mean(dist) for dist in superiority_dist],
        alpha=0.6,
        label="mean p(superior)",
    )
    plt.scatter(
        protein_activity_mean,
        [np.percentile(dist, q=0) for dist in superiority_dist],
        alpha=0.6,
        label="minimum p(superior)",
    )

    plt.xlabel("Posterior Mean Protein Activity (log scale)")
    plt.ylabel("Probability of Superiority")

    plt.legend()
    plt.title("Protein Activity vs Probability of Superiority")
    sns.despine()
    return plt.gca()


_()
```

![png](protein_estimation_46_1.webp)

This plot reveals important insights:

1. The relationship between activity and superiority is non-linear
2. Proteins with similar activities can have different probabilities of superiority depending on certainty
3. The minimum probability of superiority (0th percentile) provides a conservative measure for decision making -- a protein with a high minimum probability of superiority is more likely to be the superior candidate than a protein with a low minimum probability of superiority.

## Practical Applications Beyond Protein Screening

While we've focused on protein screening, the techniques demonstrated here apply broadly:

1. **Drug Discovery**: Compare efficacy of different compounds while accounting for batch effects
2. **Materials Science**: Evaluate materials' properties with appropriate uncertainty quantification
3. **A/B Testing**: Assess the probability that one variant truly outperforms another
4. **Clinical Trials**: Calculate the probability that a treatment is superior to alternatives

In each case, the R2D2 prior helps answer "Is our experiment measuring what we care about?" while Bayesian superiority calculation addresses "Which option is truly better?"

## Conclusion

In high-throughput screening, two questions are critical: "Are we measuring what matters?" and "Which candidates are truly superior?" Traditional approaches using point estimates and p-values fail to adequately address these questions, especially when dealing with experimental noise and multiple comparisons.

The Bayesian framework we've demonstrated offers a powerful alternative:

1. **R2D2 priors** decompose variance into interpretable components, revealing how much of our signal comes from the biological effect versus experimental artifacts. If you know how to use it, you can guide concrete improvements to experimental protocols.
2. **Bayesian superiority calculation** directly quantifies the probability that one candidate outperforms others, properly accounting for uncertainty and avoiding the pitfalls of simple rank ordering.

Together, these techniques transform screening data into actionable insights. While we've focused on protein screening, the same approach applies to any domain requiring robust comparison of multiple candidates under noisy conditions.

Bayesian methods allow us to move beyond simplistic "winners and losers" to a nuanced understanding of which candidates are most likely to succeed, with what degree of certainty, and how we can improve our measurement process itself. And more meta-level: none of this needs fancy names. It's just logic and math, applied to data. Wasn't that what statistics was supposed to be?

---
pub_date: 2025-03-30
---
twitter_handle: ericmjl
