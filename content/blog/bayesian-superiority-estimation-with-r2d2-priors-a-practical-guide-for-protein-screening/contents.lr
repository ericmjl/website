title: Bayesian Superiority Estimation with R2D2 Priors: A Practical Guide for Protein Screening
---
author: Eric J. Ma
---
body:

Recently, I've been thinking and writing a lot about statistics.
It's because good statistical practice is both under-rated, under-taught, and under-valued
amongst machine learning practitioners _and_ laboratory scientists,
and yet it underpins the ability of machine learning practitioners in life sciences
to build high performing machine learning models that accelerate decisions in the lab.
It also enables laboratory scientists to design experiments that yield interpretable,
and actionable results.

My prior experience doing the full spectrum of laboratory and computational science
is one of the reasons why it pains me to see potentially good data go to waste
due to poor experimental design and statistical analysis.
Without good statistical practice underlying the data generating process --
and by that I mean good experimental design,
and explicit quantification of uncertainty --
all ML models become equal: equally bad.

In this blog post, I want to show you how to use Bayesian methods
to tackle two critical questions:

1. Is our experimental setup actually measuring the effect we care about
   (vs. experimental noise)?
2. Which candidates are truly superior to others?

As always, I go back to my favorite example: screening proteins.
But as you read, note the generalities: they aren't specific to protein screening at all!

----

> Note: The original `marimo` notebook can be found [here](./protein_estimation.py).
> If there are discrepancies between my blog post and the original marimo notebook,
> I note that the original notebook is correct.
> To run it, download it, and then execute:
>
> ```bash
> uvx marimo edit --sandbox https://ericmjl.github.io/blog/2025/3/30/bayesian-superiority-estimation-with-r2d2-priors-a-practical-guide-for-protein-screening/protein_estimation.py
>

When screening hundreds of molecules, proteins, or interventions,
two critical questions often arise:

1. Is our experimental setup actually measuring the effect we care about
   (vs. experimental noise)?
2. Which candidates are truly superior to others?

In this tutorial, we'll tackle both challenges using a practical example:
a protein screening experiment with fluorescence readouts.
We'll show how **R2D2 priors** help interpret variance decomposition,
and how **Bayesian superiority calculation**
enables robust ranking of candidates.
Both techniques generalize to drug discovery, material science,
or any domain requiring rigorous comparison of multiple alternatives.

## The Protein Screening Example

We'll use a dataset with fluorescence measurements for over 100 proteins
across multiple experiments and replicates, with an experimental design that includes:

- A control protein present in all experiments and replicates
- "Crossover" proteins measured across all experiments
- Unique test proteins in each experiment

This design is common in high-throughput screening where measuring all proteins in all conditions is impractical.
We'll implement our analysis using PyMC, a powerful probabilistic programming framework for Bayesian modeling.


```python
import pymc as pm
```

## Generating Synthetic Data

To demonstrate our approach, we'll generate synthetic data that mimics a realistic protein screening experiment with:

- 3 experiments with 2 replicates each
- A control protein and crossover proteins present in all experiments
- Test proteins unique to each experiment
- Systematic experiment effects (batch effects)
- Replicate-level variation
- Measurement noise


```python
import numpy as np
import pandas as pd

# Set random seed for reproducibility
np.random.seed(42)

# Define parameters
n_experiments = 3
n_replicates = 2
n_proteins_per_exp = 40
n_crossover = 4

# Create protein names
control = ["Control"]
crossover_proteins = [f"Crossover_{i}" for i in range(n_crossover)]
other_proteins = [f"Protein_{i}" for i in range(100)]

# Base fluorescence values
base_values = {}
base_values["Control"] = 1000
for p in crossover_proteins:
    base_values[p] = np.random.normal(1000, 200)
for p in other_proteins:
    base_values[p] = np.random.normal(1000, 200)

# Create experiment effects
exp_effects = np.random.normal(1, 0.3, n_experiments)
rep_effects = np.random.normal(1, 0.1, (n_experiments, n_replicates))

# Generate data
data = []
for exp in range(n_experiments):
    # Select proteins for this experiment
    exp_proteins = (
        control + crossover_proteins + other_proteins[exp * 30 : (exp + 1) * 30]
    )

    for rep in range(n_replicates):
        for protein in exp_proteins:
            # Add noise and effects
            value = (
                base_values[protein]
                * exp_effects[exp]
                * rep_effects[exp, rep]
                * np.random.normal(1, 0.05)
            )

            data.append(
                {
                    "Experiment": f"Exp_{exp+1}",
                    "Replicate": f"Rep_{rep+1}",
                    "Protein": protein,
                    "Fluorescence": value,
                }
            )

# Convert to DataFrame
df = pd.DataFrame(data)
df
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Experiment</th>
      <th>Replicate</th>
      <th>Protein</th>
      <th>Fluorescence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Control</td>
      <td>1087.476281</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Crossover_0</td>
      <td>1054.176224</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Crossover_1</td>
      <td>955.647739</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Crossover_2</td>
      <td>1091.751188</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Exp_1</td>
      <td>Rep_1</td>
      <td>Crossover_3</td>
      <td>1189.344109</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>205</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_85</td>
      <td>1765.149428</td>
    </tr>
    <tr>
      <th>206</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_86</td>
      <td>1595.422298</td>
    </tr>
    <tr>
      <th>207</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_87</td>
      <td>1889.585595</td>
    </tr>
    <tr>
      <th>208</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_88</td>
      <td>1394.395041</td>
    </tr>
    <tr>
      <th>209</th>
      <td>Exp_3</td>
      <td>Rep_2</td>
      <td>Protein_89</td>
      <td>1411.831297</td>
    </tr>
  </tbody>
</table>
<p>210 rows Ã— 4 columns</p>
</div>


In the synthetic dataset we've created, we simulated:

- 3 experiments with 2 replicates each
- A control protein and 4 crossover proteins present in all experiments
- 100 other proteins distributed across experiments
- Multiplicative experiment effects (mean=1, sd=0.3)
- Multiplicative replicate effects (mean=1, sd=0.1)
- Multiplicative measurement noise (mean=1, sd=0.05)

This structure simulates a typical screening setup
where batch effects between experiments are stronger than replicate variation,
and both contribute significantly to the observed fluorescence values.
The setting mirrors real experimental challenges
where we need to separate biological signal from technical noise.

## Examining the Raw Data

Before modeling, let's visualize the control and crossover proteins to understand experimental variation:


```python
import matplotlib.pyplot as plt
import seaborn as sns

# Filter for Control and Crossover samples
mask = df["Protein"].str.contains("Control|Crossover")
filtered_df = df[mask]

# Create the swarm plot
sns.swarmplot(
    data=filtered_df, x="Experiment", y="Fluorescence", hue="Protein", size=8
)

plt.title("Activity for Controls and Crossover Samples")
plt.ylabel("Fluorescence")
plt.gca()
```



![png](protein_estimation_8_0.webp)



Notice the dramatic shift in fluorescence values across experiments.
Experiment 3 shows substantially higher fluorescence readings (around 1500-2000 units)
compared to Experiments 1 and 2 (mostly below 1300 units).

This pattern illustrates a common challenge in high-throughput screening:
significant batch effects between experiments that can mask the true biological signal.
Without accounting for these experimental factors, we might incorrectly attribute
higher activity to proteins simply because they were measured in Experiment 3.

Our modeling approach needs to account for both between-experiment and within-experiment
sources of variation to accurately compare proteins across the entire dataset.

## The R2D2 Prior for Variance Decomposition

The R2D2 prior (R-squared Dirichlet decomposition) provides an interpretable framework
for variance decomposition by placing a prior on the Bayesian coefficient of determination ($R^2$),
which then induces priors on individual parameters.

The core idea is simple but powerful:

1. Place a $\text{Beta}(a,b)$ prior on $R^2$ (the proportion of variance explained by the model)
2. Induce a prior on the global variance parameter representing total variance
3. Decompose the global variance into components via a Dirichlet distribution

This approach has key advantages over traditional hierarchical models:

- Instead of specifying arbitrary priors for each variance component separately, we control
  total explained variance through a single interpretable parameter ($R^2$)
- The variance partition becomes meaningful through the Dirichlet distribution
- The hierarchical nature ensures regularization of variance estimates

Here's our implementation of the model:


```python
def _():
    # Create categorical indices
    exp_idx = pd.Categorical(df["Experiment"]).codes
    rep_idx = pd.Categorical(df["Replicate"]).codes
    prot_idx = pd.Categorical(df["Protein"]).codes

    # Define coordinates for dimensions
    coords = {
        "experiment": df["Experiment"].unique(),
        "replicate": df["Replicate"].unique(),
        "protein": df["Protein"].unique(),
    }

    with pm.Model(coords=coords) as model:
        # Explicitly define RÂ² prior (core of R2D2)
        r_squared = pm.Beta("r_squared", alpha=1, beta=1)

        # Global parameters
        global_mean = pm.Normal(
            "global_mean", mu=7, sigma=1
        )  # log scale for fluorescence

        # Prior on total variance, which will be scaled by RÂ²
        # before being decomposed into components.
        sigma_squared = pm.HalfNormal("sigma_squared", sigma=1)

        # Global variance derived from RÂ² and residual variance
        # For normal models: W = sigmaÂ² * rÂ²/(1-rÂ²)
        global_var = pm.Deterministic(
            "global_var", sigma_squared * r_squared / (1 - r_squared)
        )
        global_sd = pm.Deterministic(
            "global_sd", pm.math.sqrt(global_var)
        )  # noqa: F841

        # R2D2 decomposition parameters
        # 4 components: experiment, replicate (nested in experiment), protein,
        # and unexplained
        props = pm.Dirichlet("props", a=np.ones(4))

        # Component variances (for interpretability)
        exp_var = pm.Deterministic("exp_var", props[0] * global_var)
        rep_var = pm.Deterministic("rep_var", props[1] * global_var)
        prot_var = pm.Deterministic("prot_var", props[2] * global_var)
        unexplained_var = pm.Deterministic("unexplained_var", props[3] * global_var)

        # Component standard deviations
        exp_sd = pm.Deterministic("exp_sd", pm.math.sqrt(exp_var))
        rep_sd = pm.Deterministic("rep_sd", pm.math.sqrt(rep_var))
        prot_sd = pm.Deterministic("prot_sd", pm.math.sqrt(prot_var))
        unexplained_sd = pm.Deterministic(
            "unexplained_sd", pm.math.sqrt(unexplained_var)
        )

        # Component effects
        exp_effect = pm.Normal("exp_effect", mu=0, sigma=exp_sd, dims="experiment")
        rep_effect = pm.Normal(
            "rep_effect", mu=0, sigma=rep_sd, dims=("experiment", "replicate")
        )
        prot_effect = pm.Normal("prot_effect", mu=0, sigma=prot_sd, dims="protein")

        # Protein activity (what we're ultimately interested in)
        prot_activity = pm.Deterministic(  # noqa: F841
            "prot_activity", global_mean + prot_effect, dims="protein"
        )

        # Expected value
        y_hat = (
            global_mean
            + exp_effect[exp_idx]
            + rep_effect[exp_idx, rep_idx]
            + prot_effect[prot_idx]
        )

        # Calculate model RÂ² directly (for verification)
        model_r2 = pm.Deterministic(  # noqa: F841
            "model_r2",
            (exp_var + rep_var + prot_var)
            / (exp_var + rep_var + prot_var + unexplained_var),
        )

        # Likelihood
        y = pm.Normal(  # noqa: F841
            "y", mu=y_hat, sigma=unexplained_sd, observed=np.log(df["Fluorescence"])
        )

        # Sample
        trace = pm.sample(
            2000, tune=1000, return_inferencedata=True, nuts_sampler="nutpie"
        )
    return model, trace

model, trace = _()
```

## Model Convergence and Variance Analysis

After fitting our model, zero divergent transitions confirm good convergence.

Let's examine how variance is partitioned across components:


```python
trace.sample_stats.diverging.sum()
```


<div><svg style="position: absolute; width: 0; height: 0; overflow: hidden">
<defs>
<symbol id="icon-database" viewBox="0 0 32 32">
<path d="M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z"></path>
<path d="M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
<path d="M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
</symbol>
<symbol id="icon-file-text2" viewBox="0 0 32 32">
<path d="M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z"></path>
<path d="M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
<path d="M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
<path d="M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
</symbol>
</defs>
</svg>
<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.
 *
 */

:root {
  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));
  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));
  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));
  --xr-border-color: var(--jp-border-color2, #e0e0e0);
  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);
  --xr-background-color: var(--jp-layout-color0, white);
  --xr-background-color-row-even: var(--jp-layout-color1, white);
  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);
}

html[theme="dark"],
html[data-theme="dark"],
body[data-theme="dark"],
body.vscode-dark {
  --xr-font-color0: rgba(255, 255, 255, 1);
  --xr-font-color2: rgba(255, 255, 255, 0.54);
  --xr-font-color3: rgba(255, 255, 255, 0.38);
  --xr-border-color: #1f1f1f;
  --xr-disabled-color: #515151;
  --xr-background-color: #111111;
  --xr-background-color-row-even: #111111;
  --xr-background-color-row-odd: #313131;
}

.xr-wrap {
  display: block !important;
  min-width: 300px;
  max-width: 700px;
}

.xr-text-repr-fallback {
  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */
  display: none;
}

.xr-header {
  padding-top: 6px;
  padding-bottom: 6px;
  margin-bottom: 4px;
  border-bottom: solid 1px var(--xr-border-color);
}

.xr-header > div,
.xr-header > ul {
  display: inline;
  margin-top: 0;
  margin-bottom: 0;
}

.xr-obj-type,
.xr-array-name {
  margin-left: 2px;
  margin-right: 10px;
}

.xr-obj-type {
  color: var(--xr-font-color2);
}

.xr-sections {
  padding-left: 0 !important;
  display: grid;
  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;
}

.xr-section-item {
  display: contents;
}

.xr-section-item input {
  display: inline-block;
  opacity: 0;
  height: 0;
}

.xr-section-item input + label {
  color: var(--xr-disabled-color);
}

.xr-section-item input:enabled + label {
  cursor: pointer;
  color: var(--xr-font-color2);
}

.xr-section-item input:focus + label {
  border: 2px solid var(--xr-font-color0);
}

.xr-section-item input:enabled + label:hover {
  color: var(--xr-font-color0);
}

.xr-section-summary {
  grid-column: 1;
  color: var(--xr-font-color2);
  font-weight: 500;
}

.xr-section-summary > span {
  display: inline-block;
  padding-left: 0.5em;
}

.xr-section-summary-in:disabled + label {
  color: var(--xr-font-color2);
}

.xr-section-summary-in + label:before {
  display: inline-block;
  content: "â–º";
  font-size: 11px;
  width: 15px;
  text-align: center;
}

.xr-section-summary-in:disabled + label:before {
  color: var(--xr-disabled-color);
}

.xr-section-summary-in:checked + label:before {
  content: "â–¼";
}

.xr-section-summary-in:checked + label > span {
  display: none;
}

.xr-section-summary,
.xr-section-inline-details {
  padding-top: 4px;
  padding-bottom: 4px;
}

.xr-section-inline-details {
  grid-column: 2 / -1;
}

.xr-section-details {
  display: none;
  grid-column: 1 / -1;
  margin-bottom: 5px;
}

.xr-section-summary-in:checked ~ .xr-section-details {
  display: contents;
}

.xr-array-wrap {
  grid-column: 1 / -1;
  display: grid;
  grid-template-columns: 20px auto;
}

.xr-array-wrap > label {
  grid-column: 1;
  vertical-align: top;
}

.xr-preview {
  color: var(--xr-font-color3);
}

.xr-array-preview,
.xr-array-data {
  padding: 0 5px !important;
  grid-column: 2;
}

.xr-array-data,
.xr-array-in:checked ~ .xr-array-preview {
  display: none;
}

.xr-array-in:checked ~ .xr-array-data,
.xr-array-preview {
  display: inline-block;
}

.xr-dim-list {
  display: inline-block !important;
  list-style: none;
  padding: 0 !important;
  margin: 0;
}

.xr-dim-list li {
  display: inline-block;
  padding: 0;
  margin: 0;
}

.xr-dim-list:before {
  content: "(";
}

.xr-dim-list:after {
  content: ")";
}

.xr-dim-list li:not(:last-child):after {
  content: ",";
  padding-right: 5px;
}

.xr-has-index {
  font-weight: bold;
}

.xr-var-list,
.xr-var-item {
  display: contents;
}

.xr-var-item > div,
.xr-var-item label,
.xr-var-item > .xr-var-name span {
  background-color: var(--xr-background-color-row-even);
  margin-bottom: 0;
}

.xr-var-item > .xr-var-name:hover span {
  padding-right: 5px;
}

.xr-var-list > li:nth-child(odd) > div,
.xr-var-list > li:nth-child(odd) > label,
.xr-var-list > li:nth-child(odd) > .xr-var-name span {
  background-color: var(--xr-background-color-row-odd);
}

.xr-var-name {
  grid-column: 1;
}

.xr-var-dims {
  grid-column: 2;
}

.xr-var-dtype {
  grid-column: 3;
  text-align: right;
  color: var(--xr-font-color2);
}

.xr-var-preview {
  grid-column: 4;
}

.xr-index-preview {
  grid-column: 2 / 5;
  color: var(--xr-font-color2);
}

.xr-var-name,
.xr-var-dims,
.xr-var-dtype,
.xr-preview,
.xr-attrs dt {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  padding-right: 10px;
}

.xr-var-name:hover,
.xr-var-dims:hover,
.xr-var-dtype:hover,
.xr-attrs dt:hover {
  overflow: visible;
  width: auto;
  z-index: 1;
}

.xr-var-attrs,
.xr-var-data,
.xr-index-data {
  display: none;
  background-color: var(--xr-background-color) !important;
  padding-bottom: 5px !important;
}

.xr-var-attrs-in:checked ~ .xr-var-attrs,
.xr-var-data-in:checked ~ .xr-var-data,
.xr-index-data-in:checked ~ .xr-index-data {
  display: block;
}

.xr-var-data > table {
  float: right;
}

.xr-var-name span,
.xr-var-data,
.xr-index-name div,
.xr-index-data,
.xr-attrs {
  padding-left: 25px !important;
}

.xr-attrs,
.xr-var-attrs,
.xr-var-data,
.xr-index-data {
  grid-column: 1 / -1;
}

dl.xr-attrs {
  padding: 0;
  margin: 0;
  display: grid;
  grid-template-columns: 125px auto;
}

.xr-attrs dt,
.xr-attrs dd {
  padding: 0;
  margin: 0;
  float: left;
  padding-right: 10px;
  width: auto;
}

.xr-attrs dt {
  font-weight: normal;
  grid-column: 1;
}

.xr-attrs dt:hover span {
  display: inline-block;
  background: var(--xr-background-color);
  padding-right: 10px;
}

.xr-attrs dd {
  grid-column: 2;
  white-space: pre-wrap;
  word-break: break-all;
}

.xr-icon-database,
.xr-icon-file-text2,
.xr-no-icon {
  display: inline-block;
  vertical-align: middle;
  width: 1em;
  height: 1.5em !important;
  stroke-width: 0;
  stroke: currentColor;
  fill: currentColor;
}
</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;diverging&#x27; ()&gt; Size: 8B
array(0)</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'diverging'</div></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-4463479b-9f49-4c6a-8025-687743d46d84' class='xr-array-in' type='checkbox' checked><label for='section-4463479b-9f49-4c6a-8025-687743d46d84' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0</span></div><div class='xr-array-data'><pre>array(0)</pre></div></div></li><li class='xr-section-item'><input id='section-adc69949-5564-4636-858b-a9577cd99d5d' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-adc69949-5564-4636-858b-a9577cd99d5d' class='xr-section-summary'  title='Expand/collapse section'>Coordinates: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-11bd5671-ae29-48a2-9b6e-4667045e7d6e' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-11bd5671-ae29-48a2-9b6e-4667045e7d6e' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-04e2fe7d-455e-432b-b246-247023813939' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-04e2fe7d-455e-432b-b246-247023813939' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>


In the ridgeline plot below, we see that unexplained variance contributes minimally to total variation,
while experiment and replicate effectsâ€”which ideally should contribute littleâ€”are actually
significant contributors to the readout variation. This serves as a metric of laboratory consistency.

Ideally, the protein variation (our biological signal of interest) should be the dominant source
of variation. This analysis suggests that improving experimental protocols to reduce batch effects
would substantially improve our signal-to-noise ratio.


```python
import arviz as az

axes_posterior_props = az.plot_posterior(trace, var_names=["props"], grid=(2, 2))
axes_posterior_props.flatten()[0].set_title("experiment")
axes_posterior_props.flatten()[2].set_title("replicate")
axes_posterior_props.flatten()[1].set_title("protein")
axes_posterior_props.flatten()[3].set_title("unexplained")
```



![png](protein_estimation_15_0.webp)



These posterior plots show the distributions of the variance components. Each represents the proportion of total variance attributed to that component. We can see clear differences in the contributions from each source.

We can also look at the total $R^2$ value, which represents the proportion of variance explained by the model:


```python
az.plot_posterior(trace, var_names=["model_r2"])
```



![png](protein_estimation_17_0.webp)



Taken together, we can interpret that the model fits the data very well (`model_r2` close to 1), but it is concerning to me that protein only explains 19% of the variation in readout, while experiment and replicate explains more than 70% of the output variation, which signals to me that the measurements are not particularly tight, and a lot could be done to control experiment-to-experiment variation.

## Protein Activity Estimates

Now that we've decomposed the variance and accounted for experimental effects,
let's examine the protein activity estimatesâ€”the "true" biological signal after
removing experimental noise:


```python
ax = az.plot_forest(trace.posterior["prot_activity"])[0]
ax.set_xlabel("log(protein activity)")
```



![png](protein_estimation_20_0.webp)



The forest plot displays posterior distributions of protein activity (log scale),
with horizontal lines representing 94% credible intervals.

A key challenge emerges: despite similar uncertainty across proteins,
overlapping credible intervals make it difficult to determine which proteins
are truly superior. Simply ranking by posterior means could lead us to prioritize
proteins with slightly higher point estimates when uncertainty makes their actual
superiority ambiguous.

This is a fundamental limitation of ranking by point estimates alone:
it fails to properly account for uncertainty. A protein with a slightly lower mean
but narrower credible intervals might be a better candidate than one with a higher
mean but wider uncertainty bounds.

## Why not calculate the effect sizes?

While effect sizes quantify difference magnitudes, they have important limitations:

1. They still have posterior distributions with uncertainty
2. They require an arbitrary reference protein
3. Scale interpretation is subjective
4. They don't directly answer "which is better?"

This is where the probability of superiority calculation shines: it integrates over
the entire posterior distribution to directly answer our key question:
"What is the probability that protein A is better than protein B?"

The probability of superiority:

1. For each posterior sample, compares protein A's activity to protein B's
2. Counts the proportion of samples where A > B
3. Results in P(A > B) - a single number from 0 to 1

This approach integrates over uncertainty, directly answers our question,
avoids arbitrary references, and produces an intuitive metric for decision-making.

Let's illustrate this with a comparison between two specific proteins:


```python
def _():
    # Get posterior samples
    prot_activity = trace.posterior["prot_activity"].values
    prot_activity_flat = prot_activity.reshape(-1, prot_activity.shape[2])

    # Get protein names
    protein_names = trace.posterior["protein"].values

    # Choose two proteins to compare
    protein1 = "Protein_12"  # A high performer
    protein2 = "Protein_66"  # Another high performer

    idx1 = np.where(protein_names == protein1)[0][0]
    idx2 = np.where(protein_names == protein2)[0][0]

    # Extract their posterior samples
    samples1 = prot_activity_flat[:, idx1]
    samples2 = prot_activity_flat[:, idx2]

    # Calculate differences for each posterior sample
    differences = samples1 - samples2

    # Calculate superiority probability
    prob_superiority = np.mean(differences > 0)

    # Plot the posterior of differences
    plt.figure(figsize=(10, 6))

    # Histogram of differences
    sns.histplot(differences, bins=30, alpha=0.6)

    # Add vertical line at zero
    plt.axvline(x=0, color="r", linestyle="--")

    # Shade the area where protein1 > protein2
    positive_mask = differences > 0
    plt.fill_between(
        np.sort(differences[positive_mask]),
        0,
        plt.gca().get_ylim()[1] / 2,  # Half height for visibility
        alpha=0.3,
        color="green",
        label=f"P({protein1} > {protein2}) = {prob_superiority:.3f}",
    )

    plt.xlabel(f"Activity Difference ({protein1} - {protein2})")
    plt.ylabel("Frequency")
    plt.title(
        "Posterior Distribution of Activity Difference "
        f"Between {protein1} and {protein2}"
    )
    return plt.legend()

_()
```



![png](protein_estimation_23_0.webp)



This visualization demonstrates the core concept. The green shaded area represents
the proportion of posterior samples where the first protein outperforms the second.
This proportion is the probability of superiority.

Rather than reducing our rich posterior distributions to point estimates or effect sizes
that still require interpretation, the superiority probability directly integrates over
all uncertainty to answer our precise question: "How likely is it that this protein is
better than that one?"

Now let's calculate this for all pairs of proteins to create a comprehensive superiority matrix:


```python
from tqdm.auto import tqdm

def _():
    n_proteins = trace.posterior["prot_activity"].shape[-1]
    prot_activity = trace.posterior["prot_activity"].values.reshape(-1, n_proteins)

    superiority_matrix = np.zeros((n_proteins, n_proteins))

    for i in tqdm(range(n_proteins)):
        for j in range(n_proteins):
            if i != j:
                superiority_matrix[i, j] = np.mean(
                    prot_activity[:, i] > prot_activity[:, j]
                )
    return superiority_matrix

superiority_matrix = _()
```


  0%|                                                                                       | 0/95 [00:00<?, ?it/s]


 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 59/95 [00:00<00:00, 585.08it/s]


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 587.17it/s]


This superiority matrix gives us, for each pair of proteins (i, j), the probability
that protein i has higher activity than protein j, incorporating all model uncertainty.

The calculation yields a probability interpretation: "There's an 85% chance that
protein A is superior to protein B" rather than the frequentist "protein A is
significantly better than protein B with p<0.05."

Let's visualize this matrix:


```python
# Create heatmap
sns.heatmap(superiority_matrix, annot=False, cmap="YlOrRd", fmt=".2f")
plt.title("Superiority Matrix")
plt.gca()
```



![png](protein_estimation_27_0.webp)



We can rank proteins by their average probability of superiority across all comparisons:


```python
def _():
    # Calculate average probability of superiority and sort proteins
    avg_superiority = superiority_matrix.mean(axis=1)
    protein_names = df["Protein"].unique()
    superiority_df = pd.DataFrame(
        {"Protein": protein_names, "Avg_Superiority": avg_superiority}
    )
    sorted_superiority = superiority_df.sort_values(
        "Avg_Superiority", ascending=False
    ).head(20)

    # Create plot
    plt.figure(figsize=(12, 6))

    # For each protein, plot individual points and mean line
    for i, protein in enumerate(sorted_superiority["Protein"]):
        protein_idx = np.where(protein_names == protein)[0][0]
        protein_probs = superiority_matrix[protein_idx]
        plt.scatter([i] * len(protein_probs), protein_probs, alpha=0.5)
        plt.hlines(avg_superiority[protein_idx], i - 0.25, i + 0.25, color="red")

    plt.xticks(
        range(len(sorted_superiority)), sorted_superiority["Protein"], rotation=90
    )
    plt.ylabel("Probability of Superiority")
    plt.title("Distribution of Superiority Probabilities by Protein")
    plt.tight_layout()
    sns.despine()
    return plt.gca()

_()
```



![png](protein_estimation_29_0.webp)



This ranking differs from what we might conclude by examining forest plots alone.
The superiority metric directly quantifies the probability that one protein
outperforms others, properly accounting for the full posterior distribution
and uncertainty in each comparison.

To better understand how protein activity relates to superiority probability,
let's compare their posterior mean activity with two superiority measures:


```python
def _():
    # Calculate probability of superiority distribution for each protein
    superiority_dist = [
        np.concatenate([superiority_matrix[i, :j], superiority_matrix[i, j + 1 :]])
        for i, j in enumerate(range(len(superiority_matrix)))
    ]

    # Get protein activity statistics from trace
    protein_activity_mean = (
        trace.posterior["prot_activity"].mean(dim=["chain", "draw"]).values
    )

    # Create scatter plot
    plt.figure(figsize=(10, 6))

    # Plot points
    plt.scatter(
        protein_activity_mean,
        [np.mean(dist) for dist in superiority_dist],
        alpha=0.6,
        label="mean p(superior)",
    )
    plt.scatter(
        protein_activity_mean,
        [np.percentile(dist, q=0) for dist in superiority_dist],
        alpha=0.6,
        label="minimum p(superior)",
    )

    plt.xlabel("Posterior Mean Protein Activity (log scale)")
    plt.ylabel("Probability of Superiority")

    plt.legend()
    plt.title("Protein Activity vs Probability of Superiority")
    sns.despine()
    return plt.gca()

_()
```



![png](protein_estimation_31_0.webp)



This plot reveals that:

1. The relationship between activity and superiority is non-linear
2. Proteins with similar activities can have different superiority probabilities
   depending on the certainty of their estimates
3. The minimum probability of superiority provides a conservative decision-making
   measureâ€”a protein with high minimum superiority is more likely to be the superior
   candidate

## Conclusion

When screening candidates, two questions are critical: "Are we measuring what matters?"
and "Which candidates are truly superior?" Traditional approaches using point estimates
and p-values inadequately address these questions when dealing with experimental noise
and multiple comparisons.

A Bayesian model with explicitly modeled terms offers a powerful alternative:

1. **R2D2 priors** decompose variance into interpretable components, revealing how
   much signal comes from the biological effect versus experimental artifacts.
   This guides concrete improvements to experimental protocols.
2. **Bayesian superiority calculation** directly quantifies the probability that
   one candidate outperforms others, properly accounting for uncertainty and
   avoiding the pitfalls of simple rank ordering.

These techniques transform screening data into actionable insights and apply to
any domain requiring robust comparison of multiple candidates under noisy conditions:
drug discovery, materials science, A/B testing, clinical trials, and more.

Bayesian methods move us beyond simplistic "winners and losers" to a nuanced
understanding of which candidates are most likely to succeed, with what degree
of certainty, and how we can improve our measurement process itself.
It lets us move beyond simplistic "winners and losers"
to a nuanced understanding of which candidates are most likely to succeed,
with what degree of certainty, and how we can improve our measurement process itself.

And more meta-level: none of this needs fancy names.
It's just logic and math, applied to data.
Wasn't that what statistics was supposed to be?

_With thanks to Jackie Valeri for sanity-checking the code while also learning the ins-and-outs of the R2D2 prior._
---
pub_date: 2025-04-03
---
twitter_handle: ericmjl
---
tags:

bayesian
r2d2
variance modelling
fluorescence
experimental design
probability of superiority
probabilistic modelling
data science
statistics
---
summary: In this blog post, I explore how to tackle experimental noise and candidate ranking in protein screening using Bayesian methods. By employing R2D2 priors, we can decompose variance into interpretable components, helping us understand the true biological signal versus experimental artifacts. Additionally, Bayesian superiority calculation allows us to quantify the probability that one protein outperforms another, providing a more robust comparison than traditional methods. These techniques are not only applicable to protein screening but also to drug discovery, materials science, and more. Are you ready to enhance your experimental insights with Bayesian logic?
