"Reflections on Developing Graph-Based Memory with AI Assistance"



Alright. This is me, uh, recording what I did with the llama bot repository. I just wanna make sure I don't forget what the development process was like, and so I just wanted to record this down while going for a walk. Okay. So I had the backstory is this. I had been nerds night so hard at the scientific Python conference, SciPy twenty twenty five by Posits CTO, Joe Cheng, j o e c h e n g. Oh, nerds night. So hard. We I had that. I went back and I decided I need to implement graph based memory. And so this is what I did then. I decided I would write the first version of graph based memory inside Lama bot, and it was not bad. I mean, I had a prototype. It was working, but it always felt a little bit fragile. And so first off, this maybe will put at the post footer or something like that. It just felt great to have Joe look at my code. I was like, wow. I've never had anyone like, we've never met. And yet he took a hard look at the code, at the design choices inside there, and I was like, I was deeply impressed by how thorough he was. Anyways, back back to the refractor. So, uh, I put up a branch. It's called conversation hyphen threads. Make sure to put backticks inside there for this. Um, and what was, I guess, cool was that, you know, I had the prototype working. I showed a few folks, but it never felt robust. So I decided, alright. Now is the time. We should go and, you know, there was a a few nights. Was it a few nights? No. No. Actually, it was on the plane ride. Right. I was I arrived at this so at the end of my two week trip, first to Tacoma then to Seattle. The end of that two week trip, decided now is the time. I need to sit down. I need to go and visit the airport. I said, alright, man. I need to sit down. I need to go and make this happen. I wanna make sure that my I wanna make sure that I have graph memory implemented in an in a proper fashion, robust robust and well, in a robust fashion. And so first things first, first thing I did was I went ahead and I asked AI to give me a design to do first a critique of the code that I had written. And then I went through it. I went through every single point bit by bit, combed through it. It was it was so, so illuminating. I had things that were very intertwined with one another. And, um, because they were so intertwined with one another, I naturally was feeling the difficulty in making those making any changes. Um, and so what AI helped with was to propose, uh, an interface, the interface that I needed to if I remember the inter the interface between components. If I remember correctly, we had we had chat memory at the high level API. Underneath the hood, there were, like oh, right. At the high level, it, uh, it proposed basically graph memory on its own and then list memory as a separate thing. And then I, uh, and then it so it had an interface for memory. Or sorry. The underlying memory structures. Um, right. It had a visualization module. Originally, it was tailored for craft. It looks nice. But then after that, we had a node selector module because I wanted, like, graph memory to have intelligent node selection, etcetera, a few things. Don't fully exactly remember what the what it was like. So then but what was what I think was really cool was what happened next. So I looked at some of the design choices, and I was like, not very satisfied with it. Right? Like, I I appreciated the first scaffold. And so I told AI, right into design doc, and I'm gonna scrutinize it bit by bit. And then what I did then was I used the design doc, and I really scrutinize it line by line. And I went back and I asked lots of questions. I was like, for example, you know, list how first off would we have two chat memory, one for linear memory and one for graph memory? And then came the natural follow-up question, which was like, well, okay. Lists are just, you know, lines. Uh, sorry. Lists are just linear graphs. So why do I need to have two separate structures? I can just have one that is, like, default to a linear graph, and then the other would involve an LLM to do the selection of which node to pass to. Right? And so then that was, like, the first design choice that I made, which is I just generalized everything to be a network x graph, memory to be a network x graph underneath the hood, and then called it a day. Right? Like, that was the right. So that was the first very first design choice that I remember iterating with AI on and figuring out that, alright. And then that's figuring out that, oh, okay. That was good. Like, I I could inject my own opinions into the matter. And then and this, by the way, we hadn't written a single line of code. We were iterating on the design doc. Any code was, uh, merely pseudo code. Right? Or or, like, prospective code written in markdown Python blocks that were that we that would give me a view on, like, okay. This is how the library is going to be used by other people. This is how the library would be used by myself if I were implementing a bot. And so that was amazing, really amazing to just be able to iterate like that. Code free, no tests. Yeah. We just went all went along with that. Okay. So then what happened next? After all of that after all of that, we went on to other design choices. I think there was some yeah. I just remember the process. Maybe I'll describe the process a little bit a little bit more because I think the process is what really makes it magical. Right? Because I had code written in markdown Python blocks. Um, markdown Python code blocks. I could actually take that and play in my head, like, okay. How would this thing look like underneath the hood? Or how would someone use it? Right? By asking very specific how questions, I could probe for a deeper understanding of, like, what was going on. And I could probe and try to make sure that, like, I've I truly understood and agreed with the design choices that were being written into the design doc. And so that was the first piece. Me first part of the process was me scrutinizing literally every line, every code block until I felt like I understood how things would be implemented. And then once I was done with that, I let the AI loose. No. Sorry. I didn't do that. Once I was done with that, I then went back and asked the AI, okay. We've got a very, very long design doc. It's at least 400, 500 lines markdown file or something like that. It was huge. I didn't have the energy or wherewithal to go and look and scrutinize everything for an overhaul for a, you know, global synthesis of what was written. So what I did then was I commanded the AI, go look for any inconsistencies that you can see within the doc. I want you to pick out all inconsistencies and surface them for me. This is taking advantage of one of the things that AI systems are really good at, which is they're good at knowledge retrieval. And we have to really hack yeah. We have to really hack at if we want to use AI systems productively, we have to hack this in such a way that we get AI to do retrieval of information or synthesis of inform of what would other buy otherwise be very tedious knowledge work. We need to get AI to help do that work for us. What this allows us then what this allowed me to do is, like, I saw at least seven or eight different points, some of which I agreed with. And keep in mind, all of this was very fresh in my mind because I just reviewed the design doc. So it's all very fresh in my mind. I had it I had a lot of detail that some for which yeah. But but then because there was a lot of detail in the design doc, it actually turned out that there were some inconsistencies, AI system, the GitHub Copilot sorry. Cursors AI surfaced them for me. I went through and combed through them one by one and gave my commentary, and we resolved them. At the end, asked it, check one more time. Double check for me. Do you see any more inconsistencies? Now while that was going on, I didn't fully offload inconsistency search to AI. Right? Like, I'm also doing a synthesis in my head and trying to make sure that I caught some stuff. And in fact, if I were to just fast forward a little bit, I caught an inconsistency in between the documentation and API where I was using a bot attribute. Sometimes it was called dot memory, and sometimes it was called dot chat underscore memory. And so I I I, you know, still put the best of my intellect to bear on the problem. That iterative process was just so flowing, so good. I was able to see, uh, how yeah. I was just able to see that, uh, in action. Alright. Um, and I went I went, like, to town. I really went to town on that one. It was so, so fun. Let's Let's see. Okay. So keep in mind, still haven't written any single lines of code. No code written. No code written yet. Still just iterating on the design doc. Look for inconsistencies twice. AI surfaced up quite a lot of them, and I gave my commentary, most of which I agreed, some of which I said ignore it. Don't matter. It's okay. I don't think it'll be consequential. And then that was, I think, two hours of iteration on the design doc. Just get it to this point after my first prototype, of course, which, by the way, surfaces for me a lesson. I wouldn't be able to critique AI the way I did if I didn't first develop a sense of taste for myself. It really relates to an old blog post an an earlier blog post that I wrote, which is that I have to earn the automation. So first prototype written by hand, I earned the automation because now I'm able to go and have have opinions and have thoughts on how things ought to be done, and so I'm not delegating critical thinking work to AI. K. Anyways, back to back to the original, the previous stuff. So so I'm done with the design doc. It's about two hours of work in total or maybe three. Um, and there's just a lot of iteration, me checking stuff. Um, once I'm done, I just do and tell the AI, alright. Go write the tests. Write all the tests. Follow the directory structure. Make sure that the directory structure of the tests matches the directory structure that we are you're proposing. Write the tests first, then I reviewed every single test. It's a lot of code review on that one. But what I think is kinda cool is that AI generated tests don't tend to be complicated. They they tend to be on the simpler side. Like, I don't see parameterized tests that use property based testing like hypothesis. Instead, I tend to see example based tests. And, actually, as a first pass as a first pass, example based tests are great. They are concrete. They're easy to grasp, and I can have confidence that if I believe the test is testing what I think it should test, then the test should pass when the implementation is actually written. So I was okay. That was cool. Not a problem. So then what happened next? After writing the tests or after having AI write the tests, I continued on and had it generate finally the code. And then I did a lot of code review. And, again, first that first, like, I was a f okay. So I was really able to critique the code because I had done the design work and prototyping work upfront. So after, you know, on and off coding on the prototype and letting it simmer in my head for about a week while in Seattle and Tacoma, and then after that, going ahead and, like, generating the design doc and then being really, really grounded in the design doc. The test code review really helped, and then the generated code, like the test code review, I was able to do it so quickly because I was so grounded in what the code was supposed to do. And then when the implementation came, the tests grounded the implementation. So it's like the documentation grounds the tests, the tests doc ground, the doc, uh, the actual source code. And then, of course, we still iterated quite a bit on the on the the tests and the code as there was still some mismatch. AI is not perfect. It's not going to one shot such a complex idea, but or with with, like, good it's not gonna one shot an idea with necessarily good high quality, well organized code, but it will help. It will do, like even if it can one shot it with, like, you know, slap together code, basically, that fulfills the requirements. But good engineering, good design, really, not engineering, good design of the code, adheres to separation of concerns, etcetera. That stuff, humans still need to steer AI to do the right thing. Okay. So now we're done with that. Okay. Alright. Now let's continue on. What happens next? I go back and forth with the AI on the code and the test. Sometimes the tests will fail, and when the test failed yeah. The the first pass, I remember, a lot of the tests failed, and I was okay with that for the following reason. Well, no. I wasn't okay with that. But a lot of the tests failed, and I figured out a way to help me iterate through batches of tests and by literally copying and pasting the, uh, copying and pasting the outputs of PyTest and then getting AI to categorize the test according to the most common failure modes, which is, like, blazing fast at doing. Like, for me to figure that out on my own, I wouldn't be able to do that in the short time. Like, it was near instantaneous. But the idea of categorizing the test was, like, key here. So if I could categorize the test, I can knock out three or four, sometimes even seven failing tests with a few small code changes. And even more so, sometimes, the failing tests actually revealed for me misunderstandings either that I had about the code or that a the AI system had about the code, and it prompted it basically forced me to make a decision about the design so that we could resolve the discordance between what was expected in the test and what was written in the code. So that was cool. Right? That was really cool. I could knock out three, four, five, sometimes seven tests at one shot and make sure that the tests were passing. And once the tests were passing, I could go on and iterate on the next round of code tests. And so with that, I'll very quickly narrowed the failing tests from, like, 20 plus tests that were failing, I think, down to maybe, like, three or four that were very individual tests that were, you know, one off either syntax errors or the like. And finally, I got it to the point where everything works dutifully. Like, all tests passed. The discordances that showed up were resolved, and I was able to ship this code with confidence. All told, I think it was, like today's today's Monday. So I flew on Sunday. Yeah. I flew on Sunday. Sunday was when I started that, and then Monday evening is when I finally got the pull request done and up to what I was expecting. And so that was that was really cool. I I I thought that was really cool just to see. Yeah. So what what are what are my what are my thoughts now? What are my thoughts now? First off, this was first off, I think the big lesson is you have to earn your automation, and I'll link to my blog post. Note for AI put a placeholder to remind me that I need to link to that blog post, earn your automation. Secondly, I was using you you really need to earn your automation without the prep work of first marinating the idea for over four months, and then finally taking a week after being nerds night to go and, um, yeah, to go and write write the implement first past implementation. And then finally, to then have two days of very compressed time where I'm just going to town even with a poor Wi Fi on JetBlue. Oh my gosh. That's a totally different story. But, yeah, it was very, very much blocked by JetBlue's Wi Fi. But it I was able to compress a lot of the actual implementation work down to two days because I had a very clear goal of what was what needed to be shipped. So for the phase where it's like, okay. I'm I'm done with the prototype. I'm done with the figuring out what actually is the problem that I'm trying to solve. Once I'm done, bam. Holy smokes. Two days to ship. That is amazing. Secondly, the use of AI as an information retrieval tool really featured here, and I think we should elaborate on this in the blog post because it was just immensely powerful. I was I was like, woah. I was go going to town on on, like, details that I didn't I was able to really deep dive into details. And I think getting AI to do self comparison and cross comparison, um, is a great way to surface up any potential issues. And what we need to do as we're a users of AI is we have to stop using AI merely as a generative tool, but as a as an aid for critical thinking. Right? The the and, you know, as I've heard many times, if in doubt, invert, whoever that quote comes from, AI helped me figure out that quote, please. But as I've heard multiple times, if in doubt, invert. And so, like, critical to the the one of the core skills of critical thinking is go and invert and figure out what happens if not. Right? So I'm like, the the the the usual lazy pattern is, what if I just assume that the usual lazy pattern is, I'll just assume the thing is correct. And that is the vibe coding definition. But one thing we should be doing with AI assistance is asking the question, what if it's not correct? Invert. What if it's not correct? If it's not correct, then perhaps, uh, one of the next follow-up logical questions is, can I get AI to tell me where it's not correct? That's me combining inversion with one of the key strengths of AI systems, which is like knowledge retrieval. Yes. AI has the problem of the needle in the haystack, but, um, for big needles in the smaller haystacks, holy smokes, it's good at finding those. And if the needle is defined as where my disc am I self contradictory or where am I discordant or where is my post not up to snuff in the sense of, uh, where's my post? Um, uh, Not self coherent. Right? Like, all the assumptions I might have about something that is in front of me that is based in text, can I use it as a tool for critical thinking? Um, so if in doubt, always invert. And now we have a lightning fast tool for helping us to invert. Right? And that I think was, like, extremely powerful. Another really powerful lesson that I want to share through the blog post. Um, let's see. Third, uh, another, I think, really cool thing was that the ability to sharpen my thinking with AI assistant. Oh, no. You know, that's that's related to inverting. So I wouldn't I wouldn't make that a a third category of thing. Those were the major things I just wanted to talk about in the blog post. That's one of the things that could really help here is if the blog posts were written in such a way that the um, yeah. If I had if I had the blog post written in such a way that it was, like, extremely accessible. Oh, right. No. Towards the end. Yeah. Towards the end. This is another thing that I wanted to say. Um, so now having gone through all of that work, what does graph based memory look like? Well, beautiful. You can have a chat conversation represented as a graph. And, um, because I now pretty much work exclusively in Marimo notebooks, I have the ability to go and run a particular, view mermaid diagrams. There's one of the visualization tools that I chose. With a mermaid diagram in Marimo notebook, it's powerful. Uh, I can actually jump around the conversation thread using the graph as, like, memory visual memory for myself to continue probing at the AI system. So, uh, I just wanted to share that as, like, a development process in which AI was used as a real partner, right, in how, uh, in the design process and and the likes. I think this works not just for one person coding solo with AI, but it works even better. I have a hypothesis that this works better if we do it to people with AI assistant, um, and up to only two people. You can't have too many cooks. You can't have too many cooks. What the way that the way that we scale, uh, this is again tied to how we work at the data DSAI, data science and artificial intelligence teams at Moderna. Early on, I instituted the practice of pair coding so that we could help each other, right, and share knowledge through pair coding. Yes. We get less done in the same amount of time, but in the long run, we're able to move faster. And this is extremely important because I can now just quickly jump onto someone else's system code base, and that shared knowledge of and shared practice of pair coding really helps with that. Now shared coding as a pair coding as a practice needs to be maintained, and I noticed recently I was starting to get isolated solo coding. Now I have a hypothesis because I actually recently in my Seattle trip, experienced this with my colleague Dan Luu, d a n, uh, l u u. I experienced the process of, like, pair coding with him with AI assistance, and we were learning tips from each other how to prompt AI. And I'm like, this is prime. This is this is, like, really great. We we have a chance to share practices for how to use AI to connect ourselves. And that is incredibly, incredibly powerful as a way of working. Because what used to be, oh, here's how you write the function. Now we can instead share how we're actually thinking. We've elevated the level at which we share our knowledge. And as Dan prompts the AI system or as I prompt the AI system, we're learning how we each other thinks in a way that is much more much more smooth and fluent and less bogged down by yeah. It's much less bogged down by it is not bogged down at all by how by the syntax or whatever. It's it's very fluent. It's operating at a higher plane than mere code and syntax. Okay. So with that, I'm gonna conclude. That's the end of my blog post. I'm gonna stop here. I hope this blog post is useful for you. It's it's it's, like, very, very, very enlightening. For me, this experience was very enlightening. Alright. Cool.
